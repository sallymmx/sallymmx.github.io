(window.webpackJsonp=window.webpackJsonp||[]).push([[30],{322:function(n,e,t){"use strict";t.r(e);var o=t(14),r=Object(o.a)({},(function(){var n=this,e=n._self._c;return e("ContentSlotsDistributor",{attrs:{"slot-key":n.$parent.slotKey}},[e("h1",{attrs:{id:"action-recognition"}},[n._v("Action Recognition")]),n._v(" "),e("h2",{attrs:{id:"news"}},[n._v("News")]),n._v(" "),e("ul",[e("li",[e("p",[n._v("[Jan, 2025] 1 paper is accepted by "),e("strong",[n._v("CVPR 2025")]),n._v("  ðŸŽ‰ðŸŽ‰ðŸŽ‰")])]),n._v(" "),e("li",[e("p",[n._v("[Jan, 2024] 1 paper is accepted by "),e("strong",[n._v("AAAI 2024 ("),e("font",{attrs:{color:"red"}},[n._v("Oral")]),n._v(")")],1),n._v("  ðŸŽ‰ðŸŽ‰ðŸŽ‰")])]),n._v(" "),e("li",[e("p",[n._v("[Nov, 2023] 1 paper is accepted by "),e("strong",[n._v("TNNLS 2023")]),n._v("  ðŸŽ‰")])]),n._v(" "),e("li",[e("p",[n._v("[July, 2023] 1 paper is accepted by "),e("strong",[n._v("ICCV 2023")]),n._v("  ðŸŽ‰")])]),n._v(" "),e("li",[e("p",[n._v("[Jan, 2023] 1 paper is accepted by "),e("strong",[n._v("AAAI 2023")]),n._v("  ðŸŽ‰")])]),n._v(" "),e("li",[e("p",[n._v("[Apr, 2022] 1 paper is accepted by "),e("strong",[n._v("TPAMI 2022")]),n._v("  ðŸŽ‰ðŸŽ‰ðŸŽ‰")])]),n._v(" "),e("li",[e("p",[n._v("[Jul, 2019] 1 paper is accepted by "),e("strong",[n._v("ICCV 2019")]),n._v(" ðŸŽ‰")])])]),n._v(" "),e("h2",{attrs:{id:"pulications"}},[n._v("Pulications")]),n._v(" "),e("ProjectCard",{attrs:{image:"/projects/cvpr2025.png",hideBorder:"true"}},[e("p",[e("strong",[n._v("Action Detail Matters: Refining Video Recognition with Local Action Queries")])]),n._v(" "),e("p",[e("strong",[n._v("Mengmeng Wang")]),n._v(", Zeyi Huang, Xiangjie Kong, Guojiang Shen, Guang Dai, Jingdong Wang, and Yong Liu.  Computer Vision and Pattern Recognition ("),e("strong",[n._v("CVPR")]),n._v("). 2025.")])]),n._v(" "),e("ProjectCard",{attrs:{image:"/projects/aaai2024.png",hideBorder:"true"}},[e("p",[e("strong",[n._v("A Multimodal, Multi-Task Adapting Framework for Video Action Recognition")])]),n._v(" "),e("p",[e("strong",[n._v("Mengmeng Wang")]),n._v(", Jiazheng Xing, Boyuan Jiang,  Jun Chen, Jianbiao Mei, Xingxing Zuo, Guang Dai, Jingdong Wang, Yong Liu*")]),n._v(" "),e("p",[n._v("Proceedings of the AAAI Conference on Artificial Intelligence ("),e("strong",[n._v("AAAI")]),n._v("), 2024, ("),e("font",{attrs:{color:"red"}},[e("strong",[n._v("Oral")])]),n._v(")")],1),n._v(" "),e("p",[n._v("["),e("a",{attrs:{href:"https://ojs.aaai.org/index.php/AAAI/article/view/28361",target:"_blank",rel:"noopener noreferrer"}},[n._v("PDF"),e("OutboundLink")],1),n._v("]")])]),n._v(" "),e("ProjectCard",{attrs:{image:"/projects/actionclip.png",hideBorder:"true"}},[e("p",[e("strong",[n._v("ActionCLIP: Adapting Language-Image Pretrained Models for Video Action Recognition")])]),n._v(" "),e("p",[e("strong",[n._v("Mengmeng Wang")]),n._v(", Jiazheng Xing, Jianbiao Mei, Yong Liu, Yunliang Jiang")]),n._v(" "),e("p",[n._v("IEEE Transactions on Neural Networks and Learning Systems ("),e("strong",[n._v("TNNLS")]),n._v("), 2023")]),n._v(" "),e("p",[n._v("["),e("a",{attrs:{href:"https://github.com/sallymmx/ActionCLIP",target:"_blank",rel:"noopener noreferrer"}},[n._v("Code"),e("OutboundLink")],1),n._v("] ["),e("a",{attrs:{href:"https://arxiv.org/abs/2109.08472",target:"_blank",rel:"noopener noreferrer"}},[n._v("PDF"),e("OutboundLink")],1),n._v("]")])]),n._v(" "),e("ProjectCard",{attrs:{image:"/projects/few1.png",hideBorder:"true"}},[e("p",[e("strong",[n._v("Boosting Few-shot Action Recognition with Graph-guided Hybrid Matching")])]),n._v(" "),e("p",[n._v("Jiazheng Xing, "),e("strong",[n._v("Mengmeng Wang")]),n._v(", Yudi Ruan, Bofan Chen, Yaowei Guo, Boyu Mu, Guang Dai, Jingdong Wang, Yong Liu")]),n._v(" "),e("p",[n._v("Proceedings of the IEEE/CVF International Conference on Computer Vision ("),e("strong",[n._v("ICCV")]),n._v("), 2023")]),n._v(" "),e("p",[n._v("["),e("a",{attrs:{href:"http://openaccess.thecvf.com/content/ICCV2023/html/Xing_Boosting_Few-shot_Action_Recognition_with_Graph-guided_Hybrid_Matching_ICCV_2023_paper.html",target:"_blank",rel:"noopener noreferrer"}},[n._v("PDF"),e("OutboundLink")],1),n._v("]")])]),n._v(" "),e("ProjectCard",{attrs:{image:"/projects/pami2022.png",hideBorder:"true"}},[e("p",[e("strong",[n._v("Learning SpatioTemporal and Motion Features in a Unified 2D Network for Action Recognition")])]),n._v(" "),e("p",[e("strong",[n._v("Mengmeng Wang")]),n._v(", Jiazheng Xing, Jing Su,  Jun Chen, Yong Liu*")]),n._v(" "),e("p",[n._v("IEEE Transactions on Pattern Analysis and Machine Intelligence ("),e("strong",[n._v("TPAMI")]),n._v("), 2022")]),n._v(" "),e("p",[n._v("["),e("a",{attrs:{href:"https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=VSRnUiUAAAAJ&sortby=pubdate&citation_for_view=VSRnUiUAAAAJ:maZDTaKrznsC",target:"_blank",rel:"noopener noreferrer"}},[n._v("PDF"),e("OutboundLink")],1),n._v("]")])]),n._v(" "),e("ProjectCard",{attrs:{image:"/projects/STM.png",hideBorder:"true"}},[e("p",[e("strong",[n._v("STM: SpatioTemporal and motion encoding for action recognition")])]),n._v(" "),e("p",[n._v("Boyuan Jiang,  "),e("strong",[n._v("Mengmeng Wang")]),n._v(" *, Weihao Gan, Wei Wu, Junjie Yan.")]),n._v(" "),e("p",[n._v("Proceedings of the IEEE International Conference on Computer Vision ("),e("strong",[n._v("ICCV")]),n._v("). 2019")]),n._v(" "),e("p",[n._v("["),e("a",{attrs:{href:"https://openaccess.thecvf.com/content_ICCV_2019/papers/Jiang_STM_SpatioTemporal_and_Motion_Encoding_for_Action_Recognition_ICCV_2019_paper.pdf",target:"_blank",rel:"noopener noreferrer"}},[n._v("PDF"),e("OutboundLink")],1),n._v("]")])])],1)}),[],!1,null,null,null);e.default=r.exports}}]);