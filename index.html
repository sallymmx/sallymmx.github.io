<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Mengmeng Wang</title>
    <meta name="generator" content="VuePress 1.9.10">
    <link rel="icon" href="/logo.png">
    <meta name="description" content="The description of the site.">
    
    <link rel="preload" href="/assets/css/0.styles.c1511f5c.css" as="style"><link rel="preload" href="/assets/js/app.a0b7ae00.js" as="script"><link rel="preload" href="/assets/js/2.17befcb1.js" as="script"><link rel="preload" href="/assets/js/1.4fc49fe1.js" as="script"><link rel="preload" href="/assets/js/24.cba5baec.js" as="script"><link rel="preload" href="/assets/js/20.7f3ae6da.js" as="script"><link rel="preload" href="/assets/js/21.0ee99fce.js" as="script"><link rel="prefetch" href="/assets/js/10.6fac66c2.js"><link rel="prefetch" href="/assets/js/11.853abdab.js"><link rel="prefetch" href="/assets/js/12.1c83fcb7.js"><link rel="prefetch" href="/assets/js/13.67952611.js"><link rel="prefetch" href="/assets/js/14.cffeb6ce.js"><link rel="prefetch" href="/assets/js/15.e004d45f.js"><link rel="prefetch" href="/assets/js/16.58a5b5fd.js"><link rel="prefetch" href="/assets/js/17.a2775924.js"><link rel="prefetch" href="/assets/js/18.2de3d8f0.js"><link rel="prefetch" href="/assets/js/19.fbd5f1db.js"><link rel="prefetch" href="/assets/js/22.cfe869c1.js"><link rel="prefetch" href="/assets/js/23.9b2e3461.js"><link rel="prefetch" href="/assets/js/25.2f7930e4.js"><link rel="prefetch" href="/assets/js/26.7747f0e6.js"><link rel="prefetch" href="/assets/js/27.51049fe1.js"><link rel="prefetch" href="/assets/js/28.febdde31.js"><link rel="prefetch" href="/assets/js/29.45dcae00.js"><link rel="prefetch" href="/assets/js/3.4563d256.js"><link rel="prefetch" href="/assets/js/30.f5fc13eb.js"><link rel="prefetch" href="/assets/js/31.8e2c4c5f.js"><link rel="prefetch" href="/assets/js/32.9524d6fe.js"><link rel="prefetch" href="/assets/js/33.e60983bf.js"><link rel="prefetch" href="/assets/js/4.c15fd794.js"><link rel="prefetch" href="/assets/js/5.f8d772f8.js"><link rel="prefetch" href="/assets/js/6.c32ccf62.js"><link rel="prefetch" href="/assets/js/7.9372364e.js"><link rel="prefetch" href="/assets/js/vendors~docsearch.11055bee.js">
    <link rel="stylesheet" href="/assets/css/0.styles.c1511f5c.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar home-page"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" aria-current="page" class="home-link router-link-exact-active router-link-active"><!----> <span class="site-name">Mengmeng Wang</span></a> <div class="links"><!----> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" aria-current="page" class="nav-link router-link-exact-active router-link-active">
  Home
</a></div><div class="nav-item"><a href="/publications/" class="nav-link">
  Publications
</a></div><div class="nav-item"><a href="/projects/" class="nav-link">
  Projects
</a></div><div class="nav-item"><a href="/about/" class="nav-link">
  About
</a></div><div class="nav-item"><a href="https://www.zhihu.com/column/visual-tracking" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Blog
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/" aria-current="page" class="nav-link router-link-exact-active router-link-active">
  Home
</a></div><div class="nav-item"><a href="/publications/" class="nav-link">
  Publications
</a></div><div class="nav-item"><a href="/projects/" class="nav-link">
  Projects
</a></div><div class="nav-item"><a href="/about/" class="nav-link">
  About
</a></div><div class="nav-item"><a href="https://www.zhihu.com/column/visual-tracking" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Blog
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav>  <!----> </aside> <main class="page"> <div class="theme-default-content content__default"><div class="profile"><div class="image"><img src="/mmw3.jpg" alt></div> <div class="info"><div class="name">
      Mengmeng Wang | 王蒙蒙
    </div> <div class="bio"><p>Zhejiang University of Technology</p></div> <div class="socials"><div><a href="https://scholar.google.com/citations?user=VSRnUiUAAAAJ&amp;hl=zh-CN" target="_blank"><img src="/icons/googlescholar.svg" alt="scholar" title="scholar"></a></div><div><a href="https://www.zhihu.com/people/wang-xi-60-66" target="_blank"><img src="/icons/zhihu.svg" alt="Zhihu" title="Zhihu"></a></div><div><a href="https://github.com/sallymmx" target="_blank"><img src="/icons/github.svg" alt="github" title="github"></a></div></div> <div class="contact"><div title="Contact me" class="email">mengmewang@gmail.com</div></div> <!----></div></div> <h2 id="about-me">About Me</h2> <p>I have attended the College of Computer Science &amp; Technology,  Zhejiang University of Technology as an Associate Professor in 2024. I graduated from Zhejiang University with a PhD, supervised by Professor Yong Liu, in 2024. My research area includes Robotic Embodied Intelligence, Compute Vision and Multi-media Technology, specifically including video action recognition, robotic manipulation, object tracking, object detection, depth estimation, text-to-image editing and so on. 💫</p> <p>My works have been published on top computer vision transactions/conferences (TPAMI, TIP, TMM, CVPR, ICCV, ICLR, AAAI etc) and top robotic conferences (ICRA, IROS).</p> <h2 id="news">News</h2> <ul><li>[2025] 4 papers are accepted by <strong>IJCAL 2025</strong>  🎉</li> <li>[2025] 1 paper is accepted by <strong>ICML 2025</strong>  🎉</li> <li>[2025] 2 papers are accepted by <strong>CVPR 2025</strong>  🎉</li> <li>[2025] I will serve as an <font color="red">Area Chair</font> for ICCV 2025  🎉</li> <li>[2025] 1 paper is accepted by <strong>ICLR 2025</strong>  🎉</li> <li>[2025] 1 paper is accepted by <strong>AAAI 2025</strong>  🎉</li> <li>[2024] 3 paper is accepted by <strong>NeurIPS 2024 (One <font color="red">Oral</font>)</strong>  🎉🎉</li> <li>[2024] 1 paper is accepted by <strong>CVPR 2024</strong>  🎉</li> <li>[2024] I will serve as a PC of IEEE BIBM 2024.</li> <li>[2024] 1 paper is accepted by <strong>ACM MM 2024</strong> 🎉</li> <li>[2024] 1 paper is accepted by <strong>ICLR 2024</strong>  🎉</li> <li>[2024] 2 papers are accepted by <strong>AAAI 2024 (One <font color="red">Oral</font>)</strong>  🎉🎉🎉</li> <li>[Dec, 2023] 1 paper is accepted by <strong>TPAMI 2023</strong>  🎉🎉</li> <li>[July, 2023] 2 papers are accepted by <strong>ICCV 2023</strong>  🎉</li> <li>[Nov, 2023] 1 paper is accepted by <strong>TNNLS 2023</strong>  🎉</li> <li>[Jan, 2023] 1 paper is accepted by <strong>AAAI 2023</strong>  🎉</li> <li>[Sep, 2022] 1 paper is accepted by <strong>TIP 2022</strong>  🎉</li> <li>[Jul, 2022]   **First Prize of Science and Technology Progress of Zhejiang Province (浙江省科技进步一等奖) **🎉</li> <li>[Jul, 2022] 1 paper is accepted by <strong>ECCV 2022</strong>  🎉</li> <li>[Apr, 2022] 1 paper is accepted by <strong>TPAMI 2022</strong>  🎉🎉🎉</li></ul> <h2 id="education-experiences">Education &amp; Experiences</h2> <ul><li><strong>Zhejiang University of Technology</strong> Associate Professor,from 2024</li> <li><strong>Zhejiang University</strong> Bachelor, Master, PhD</li> <li><strong>SenseTime Smart City Group</strong>  [2018-2020]  Researcher</li></ul> <h2 id="publications">Publications</h2> <p><a href="/publications/">→ Full list</a></p> <div class="md-card"><div class="card-image"><img src="/projects/iclr2025.jpg" alt></div> <div class="card-content"><p><strong>Manifold Constraint Reduces Exposure Bias in Accelerated Diffusion Sampling</strong></p> <p>Yuzhe YAO, Jun Chen, Zeyi Huang, Haonan Lin, <strong>Mengmeng Wang</strong>*, Guang Dai, Jingdong Wang</p> <p>International Conference on Learning Representations (<strong>ICLR</strong>), 2025</p> <p>[<a href="https://openreview.net/pdf?id=5xmXUwDxep" target="_blank" rel="noopener noreferrer">PDF<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>]</p></div></div> <div class="md-card"><div class="card-image"><img src="/projects/neurips2024.jpg" alt></div> <div class="card-content"><p><strong>Schedule Your Edit: A Simple yet Effective Diffusion Noise Schedule for Image Editing</strong></p> <p>Haonan Lin, Yan Chen, Jiahao Wang, Wenbin An, <strong>Mengmeng Wang</strong>*, Feng Tian, Yong Liu, Guang Dai, Jingdong Wang, QianYing Wang</p> <p>Neural Information Processing Systems (<strong>NeurIPS</strong>), 2024</p> <p>[<a href="https://lonelvino.github.io/SYE/#/" target="_blank" rel="noopener noreferrer">Code<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>]  [<a href="https://openreview.net/pdf?id=Yu6cDt7q9Z" target="_blank" rel="noopener noreferrer">PDF<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>]</p></div></div> <div class="md-card"><div class="card-image"><img src="/projects/AAAI2024.png" alt></div> <div class="card-content"><p><strong>A Multimodal, Multi-Task Adapting Framework for Video Action Recognition</strong></p> <p><strong>Mengmeng Wang</strong>, Jiazheng Xing, Boyuan Jiang,  Jun Chen, Jianbiao Mei, Xingxing Zuo, Guang Dai, Jingdong Wang, Yong Liu*</p> <p>Proceedings of the AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2024, (<font color="red"><strong>Oral</strong></font>)</p> <p>[<a href="https://github.com/sallymmx/m2clip" target="_blank" rel="noopener noreferrer">Code<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>]  [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/28361" target="_blank" rel="noopener noreferrer">PDF<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>]</p></div></div> <div class="md-card"><div class="card-image"><img src="/projects/actionclip.png" alt></div> <div class="card-content"><p><strong>ActionCLIP: Adapting Language-Image Pretrained Models for Video Action Recognition</strong></p> <p><strong>Mengmeng Wang</strong>, Jiazheng Xing, Jianbiao Mei, Yong Liu, Yunliang Jiang</p> <p>IEEE Transactions on Neural Networks and Learning Systems (<strong>TNNLS</strong>), 2023</p> <p>[<a href="https://github.com/sallymmx/ActionCLIP" target="_blank" rel="noopener noreferrer">Code<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>] [<a href="https://arxiv.org/abs/2109.08472" target="_blank" rel="noopener noreferrer">PDF<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>]</p></div></div> <div class="md-card"><div class="card-image"><img src="/projects/corpnet.png" alt></div> <div class="card-content"><p><strong>Correlation pyramid network for 3d single object tracking</strong></p> <p><strong>Mengmeng Wang</strong>, Teli Ma, Xingxing Zuo, Jiajun Lv, Yong Liu</p> <p>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023</p> <p>[<a href="https://openaccess.thecvf.com/content/CVPR2023W/E2EAD/html/Wang_Correlation_Pyramid_Network_for_3D_Single_Object_Tracking_CVPRW_2023_paper.html" target="_blank" rel="noopener noreferrer">PDF<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>]</p></div></div> <div class="md-card"><div class="card-image"><img src="/projects/pami2022.png" alt></div> <div class="card-content"><p><strong>Learning SpatioTemporal and Motion Features in a Unified 2D Network for Action Recognition</strong></p> <p><strong>Mengmeng Wang</strong>, Jiazheng Xing, Jing Su,  Jun Chen, Yong Liu*</p> <p>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>), 2022</p> <p>[<a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=zh-CN&amp;user=VSRnUiUAAAAJ&amp;sortby=pubdate&amp;citation_for_view=VSRnUiUAAAAJ:maZDTaKrznsC" target="_blank" rel="noopener noreferrer">PDF<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>]</p></div></div> <div class="md-card"><div class="card-image"><img src="/projects/iccv2023.png" alt></div> <div class="card-content"><p><strong>Synchronize Feature Extracting and Matching: A Single Branch Framework for 3D Object Tracking</strong></p> <p>Teli Ma, <strong>Mengmeng Wang</strong>, Jimin Xiao, Huifeng Wu, Yong Liu</p> <p>Proceedings of the IEEE/CVF International Conference on Computer Vision (<strong>ICCV</strong>), 2023</p> <p>[<a href="http://openaccess.thecvf.com/content/ICCV2023/html/Ma_Synchronize_Feature_Extracting_and_Matching_A_Single_Branch_Framework_for_ICCV_2023_paper.html" target="_blank" rel="noopener noreferrer">PDF<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>]</p></div></div> <div class="md-card"><div class="card-image"><img src="/projects/few1.png" alt></div> <div class="card-content"><p><strong>Boosting Few-shot Action Recognition with Graph-guided Hybrid Matching</strong></p> <p>Jiazheng Xing, <strong>Mengmeng Wang</strong>, Yudi Ruan, Bofan Chen, Yaowei Guo, Boyu Mu, Guang Dai, Jingdong Wang, Yong Liu</p> <p>Proceedings of the IEEE/CVF International Conference on Computer Vision (<strong>ICCV</strong>), 2023</p> <p>[<a href="http://openaccess.thecvf.com/content/ICCV2023/html/Xing_Boosting_Few-shot_Action_Recognition_with_Graph-guided_Hybrid_Matching_ICCV_2023_paper.html" target="_blank" rel="noopener noreferrer">PDF<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>]</p></div></div> <div class="md-card"><div class="card-image"><img src="/projects/HRNet.gif" alt></div> <div class="card-content"><p><strong>HR-Depth : High Resolution Self-Supervised Monocular Depth Estimation</strong></p> <p>Xiaoyang Lyu, Liang Liu, <strong>Mengmeng Wang</strong>, Xin Kong, Lina Liu, Yong Liu*, Xinxin Chen, Yi Yuan</p> <p>The Association for the Advance of Artificial Intelligence (<strong>AAAI</strong>), 2021</p> <p>[<a href="https://github.com/shawLyu/HR-Depth" target="_blank" rel="noopener noreferrer">Code<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>] [<a href="https://arxiv.org/pdf/2012.07356.pdf" target="_blank" rel="noopener noreferrer">PDF<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>]</p></div></div> <div class="md-card"><div class="card-image"><img src="/projects/AAAI2020Head.png" alt></div> <div class="card-content"><p><strong>FDN: Feature Decoupling Network for Head Pose Estimation</strong></p> <p>Hao Zhang, <strong>Mengmeng Wang</strong>, Yong Liu Yi Yuan</p> <p>The Association for the Advance of Artificial Intelligence (<strong>AAAI</strong>), 2020</p> <p>[<a href="https://ojs.aaai.org/index.php/AAAI/article/view/6974/6828" target="_blank" rel="noopener noreferrer">PDF<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>]</p></div></div> <div class="md-card"><div class="card-image"><img src="/projects/STM.png" alt></div> <div class="card-content"><p><strong>STM: SpatioTemporal and motion encoding for action recognition</strong></p> <p>Boyuan Jiang,  <strong>Mengmeng Wang</strong> *, Weihao Gan, Wei Wu, Junjie Yan.</p> <p>Proceedings of the IEEE International Conference on Computer Vision (<strong>ICCV</strong>). 2019</p> <p>[<a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Jiang_STM_SpatioTemporal_and_Motion_Encoding_for_Action_Recognition_ICCV_2019_paper.pdf" target="_blank" rel="noopener noreferrer">PDF<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>]</p></div></div> <div class="md-card"><div class="card-image"><img src="/projects/tmech.png" alt></div> <div class="card-content"><p><strong>Accurate and Real-time 3D Tracking for the Following Robots by Fusing Vision and Ultra-sonar Information</strong></p> <p><strong>Mengmeng Wang</strong>, Yong Liu*, Daobilige Su, Yufan Liao, Lei Shi and Jinhong Xu.</p> <p>IEEE/ASME Transactions on Mechatronics, 2018</p> <p>[<a href="https://ieeexplore.ieee.org/document/8327519" target="_blank" rel="noopener noreferrer">PDF<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>] [<a href="https://zhuanlan.zhihu.com/p/34920240" target="_blank" rel="noopener noreferrer">BLOG<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>]</p></div></div> <div class="md-card"><div class="card-image"><img src="/projects/LMCF.png" alt></div> <div class="card-content"><p><strong>Large Margin Object Tracking with Circulant Feature Map</strong></p> <p><strong>Mengmeng Wang</strong>, Yong Liu∗, Zeyi Huang</p> <p>IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2017</p> <p>[<a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_Large_Margin_Object_CVPR_2017_paper.pdf" target="_blank" rel="noopener noreferrer">PDF<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>] [<a href="https://zhuanlan.zhihu.com/p/25761718" target="_blank" rel="noopener noreferrer">BLOG<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>] [<a href="https://github.com/sallymmx/LMCF" target="_blank" rel="noopener noreferrer">Results<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>]</p></div></div> <h2 id="awards-honors">Awards &amp; Honors</h2> <ul><li><a href="https://www.cie-info.org.cn/site/content/4047.html" target="_blank" rel="noopener noreferrer"><strong>Excellent master dissertation</strong><span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> <strong>of <em>Chinese Institute of Electronics (中国电子学会)</em>, 2020</strong> 🎉</li> <li><strong>Excellent Master Thesis, Zhejiang Province, 2020</strong></li> <li><strong>Outstanding Graduate, Zhejiang Province &amp; Zhejiang University, 2018</strong></li> <li><strong>Google Women Techmaker, Global,  2017</strong> 🎉</li> <li><strong>National Scholarship</strong></li></ul></div> <footer class="page-edit"><!----> <!----></footer> <!----> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.a0b7ae00.js" defer></script><script src="/assets/js/2.17befcb1.js" defer></script><script src="/assets/js/1.4fc49fe1.js" defer></script><script src="/assets/js/24.cba5baec.js" defer></script><script src="/assets/js/20.7f3ae6da.js" defer></script><script src="/assets/js/21.0ee99fce.js" defer></script>
  </body>
</html>
