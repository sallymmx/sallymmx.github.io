<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Action Recognition | Mengmeng Wang</title>
    <meta name="generator" content="VuePress 1.9.10">
    <link rel="icon" href="/logo.png">
    <meta name="description" content="The description of the site.">
    
    <link rel="preload" href="/assets/css/0.styles.c1511f5c.css" as="style"><link rel="preload" href="/assets/js/app.9a3776bf.js" as="script"><link rel="preload" href="/assets/js/2.17befcb1.js" as="script"><link rel="preload" href="/assets/js/1.4fc49fe1.js" as="script"><link rel="preload" href="/assets/js/30.eba3060f.js" as="script"><link rel="preload" href="/assets/js/21.0ee99fce.js" as="script"><link rel="prefetch" href="/assets/js/10.6fac66c2.js"><link rel="prefetch" href="/assets/js/11.853abdab.js"><link rel="prefetch" href="/assets/js/12.1c83fcb7.js"><link rel="prefetch" href="/assets/js/13.67952611.js"><link rel="prefetch" href="/assets/js/14.cffeb6ce.js"><link rel="prefetch" href="/assets/js/15.e004d45f.js"><link rel="prefetch" href="/assets/js/16.58a5b5fd.js"><link rel="prefetch" href="/assets/js/17.a2775924.js"><link rel="prefetch" href="/assets/js/18.2de3d8f0.js"><link rel="prefetch" href="/assets/js/19.fbd5f1db.js"><link rel="prefetch" href="/assets/js/20.7f3ae6da.js"><link rel="prefetch" href="/assets/js/22.cfe869c1.js"><link rel="prefetch" href="/assets/js/23.9b2e3461.js"><link rel="prefetch" href="/assets/js/24.2129c57a.js"><link rel="prefetch" href="/assets/js/25.2f7930e4.js"><link rel="prefetch" href="/assets/js/26.264cf966.js"><link rel="prefetch" href="/assets/js/27.51049fe1.js"><link rel="prefetch" href="/assets/js/28.78ee0fd4.js"><link rel="prefetch" href="/assets/js/29.a5c0f12f.js"><link rel="prefetch" href="/assets/js/3.4563d256.js"><link rel="prefetch" href="/assets/js/31.a12874e4.js"><link rel="prefetch" href="/assets/js/32.e86f73e6.js"><link rel="prefetch" href="/assets/js/4.c15fd794.js"><link rel="prefetch" href="/assets/js/5.f8d772f8.js"><link rel="prefetch" href="/assets/js/6.c32ccf62.js"><link rel="prefetch" href="/assets/js/7.9372364e.js"><link rel="prefetch" href="/assets/js/vendors~docsearch.11055bee.js">
    <link rel="stylesheet" href="/assets/css/0.styles.c1511f5c.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><!----> <span class="site-name">Mengmeng Wang</span></a> <div class="links"><!----> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link">
  Home
</a></div><div class="nav-item"><a href="/publications/" class="nav-link">
  Publications
</a></div><div class="nav-item"><a href="/projects/" class="nav-link router-link-active">
  Projects
</a></div><div class="nav-item"><a href="/about/" class="nav-link">
  About
</a></div><div class="nav-item"><a href="https://www.zhihu.com/column/visual-tracking" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Blog
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link">
  Home
</a></div><div class="nav-item"><a href="/publications/" class="nav-link">
  Publications
</a></div><div class="nav-item"><a href="/projects/" class="nav-link router-link-active">
  Projects
</a></div><div class="nav-item"><a href="/about/" class="nav-link">
  About
</a></div><div class="nav-item"><a href="https://www.zhihu.com/column/visual-tracking" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Blog
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group depth-0"><p class="sidebar-heading open"><span>Projects</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/projects/" aria-current="page" class="sidebar-link">Introduction</a></li><li><a href="/projects/visual-tracking.html" class="sidebar-link">Visual Tracking</a></li><li><a href="/projects/depth-estimation.html" class="sidebar-link">Depth Estimation</a></li><li><a href="/projects/action-recognition.html" aria-current="page" class="active sidebar-link">Action Recognition</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/projects/action-recognition.html#news" class="sidebar-link">News</a></li><li class="sidebar-sub-header"><a href="/projects/action-recognition.html#pulications" class="sidebar-link">Pulications</a></li></ul></li></ul></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="action-recognition">Action Recognition</h1> <h2 id="news">News</h2> <ul><li><p>[Jan, 2024] 1 paper is accepted by <strong>AAAI 2024 (<font color="red">Oral</font>)</strong>  ğŸ‰ğŸ‰ğŸ‰</p></li> <li><p>[Nov, 2023] 1 paper is accepted by <strong>TNNLS 2023</strong>  ğŸ‰</p></li> <li><p>[July, 2023] 1 paper is accepted by <strong>ICCV 2023</strong>  ğŸ‰</p></li> <li><p>[Jan, 2023] 1 paper is accepted by <strong>AAAI 2023</strong>  ğŸ‰</p></li> <li><p>[Apr, 2022] 1 paper is accepted by <strong>TPAMI 2022</strong>  ğŸ‰ğŸ‰ğŸ‰</p></li> <li><p>[Jul, 2019] 1 paper is accepted by <strong>ICCV 2019</strong> ğŸ‰</p></li></ul> <h2 id="pulications">Pulications</h2> <div class="md-card"><div class="card-image"><img src="/projects/aaai2024.png" alt></div> <div class="card-content"><p><strong>A Multimodal, Multi-Task Adapting Framework for Video Action Recognition</strong></p> <p><strong>Mengmeng Wang</strong>, Jiazheng Xing, Boyuan Jiang,  Jun Chen, Jianbiao Mei, Xingxing Zuo, Guang Dai, Jingdong Wang, Yong Liu*</p> <p>Proceedings of the AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2024, (<font color="red"><strong>Oral</strong></font>)</p> <p>[<a href="https://ojs.aaai.org/index.php/AAAI/article/view/28361" target="_blank" rel="noopener noreferrer">PDF<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>]</p></div></div> <div class="md-card"><div class="card-image"><img src="/projects/actionclip.png" alt></div> <div class="card-content"><p><strong>ActionCLIP: Adapting Language-Image Pretrained Models for Video Action Recognition</strong></p> <p><strong>Mengmeng Wang</strong>, Jiazheng Xing, Jianbiao Mei, Yong Liu, Yunliang Jiang</p> <p>IEEE Transactions on Neural Networks and Learning Systems (<strong>TNNLS</strong>), 2023</p> <p>[<a href="https://github.com/sallymmx/ActionCLIP" target="_blank" rel="noopener noreferrer">Code<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>] [<a href="https://arxiv.org/abs/2109.08472" target="_blank" rel="noopener noreferrer">PDF<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>]</p></div></div> <div class="md-card"><div class="card-image"><img src="/projects/few1.png" alt></div> <div class="card-content"><p><strong>Boosting Few-shot Action Recognition with Graph-guided Hybrid Matching</strong></p> <p>Jiazheng Xing, <strong>Mengmeng Wang</strong>, Yudi Ruan, Bofan Chen, Yaowei Guo, Boyu Mu, Guang Dai, Jingdong Wang, Yong Liu</p> <p>Proceedings of the IEEE/CVF International Conference on Computer Vision (<strong>ICCV</strong>), 2023</p> <p>[<a href="http://openaccess.thecvf.com/content/ICCV2023/html/Xing_Boosting_Few-shot_Action_Recognition_with_Graph-guided_Hybrid_Matching_ICCV_2023_paper.html" target="_blank" rel="noopener noreferrer">PDF<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>]</p></div></div> <div class="md-card"><div class="card-image"><img src="/projects/pami2022.png" alt></div> <div class="card-content"><p><strong>Learning SpatioTemporal and Motion Features in a Unified 2D Network for Action Recognition</strong></p> <p><strong>Mengmeng Wang</strong>, Jiazheng Xing, Jing Su,  Jun Chen, Yong Liu*</p> <p>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>), 2022</p> <p>[<a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=zh-CN&amp;user=VSRnUiUAAAAJ&amp;sortby=pubdate&amp;citation_for_view=VSRnUiUAAAAJ:maZDTaKrznsC" target="_blank" rel="noopener noreferrer">PDF<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>]</p></div></div> <div class="md-card"><div class="card-image"><img src="/projects/STM.png" alt></div> <div class="card-content"><p><strong>STM: SpatioTemporal and motion encoding for action recognition</strong></p> <p>Boyuan Jiang,  <strong>Mengmeng Wang</strong> *, Weihao Gan, Wei Wu, Junjie Yan.</p> <p>Proceedings of the IEEE International Conference on Computer Vision (<strong>ICCV</strong>). 2019</p> <p>[<a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Jiang_STM_SpatioTemporal_and_Motion_Encoding_for_Action_Recognition_ICCV_2019_paper.pdf" target="_blank" rel="noopener noreferrer">PDF<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>]</p></div></div></div> <footer class="page-edit"><!----> <!----></footer> <div class="page-nav"><p class="inner"><span class="prev">
      â†
      <a href="/projects/depth-estimation.html" class="prev">
        Depth Estimation
      </a></span> <!----></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.9a3776bf.js" defer></script><script src="/assets/js/2.17befcb1.js" defer></script><script src="/assets/js/1.4fc49fe1.js" defer></script><script src="/assets/js/30.eba3060f.js" defer></script><script src="/assets/js/21.0ee99fce.js" defer></script>
  </body>
</html>
