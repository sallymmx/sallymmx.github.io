<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Mengmeng Wang</title>
    <meta name="generator" content="VuePress 1.9.10">
    <link rel="icon" href="/logo.png">
    <meta name="description" content="The description of the site.">
    
    <link rel="preload" href="/assets/css/0.styles.13b65860.css" as="style"><link rel="preload" href="/assets/js/app.5f4e9a42.js" as="script"><link rel="preload" href="/assets/js/2.17befcb1.js" as="script"><link rel="preload" href="/assets/js/1.4fc49fe1.js" as="script"><link rel="preload" href="/assets/js/26.264cf966.js" as="script"><link rel="prefetch" href="/assets/js/10.6fac66c2.js"><link rel="prefetch" href="/assets/js/11.853abdab.js"><link rel="prefetch" href="/assets/js/12.1c83fcb7.js"><link rel="prefetch" href="/assets/js/13.67952611.js"><link rel="prefetch" href="/assets/js/14.cffeb6ce.js"><link rel="prefetch" href="/assets/js/15.e004d45f.js"><link rel="prefetch" href="/assets/js/16.58a5b5fd.js"><link rel="prefetch" href="/assets/js/17.a2775924.js"><link rel="prefetch" href="/assets/js/18.2de3d8f0.js"><link rel="prefetch" href="/assets/js/19.fbd5f1db.js"><link rel="prefetch" href="/assets/js/20.87d243a4.js"><link rel="prefetch" href="/assets/js/21.10d257ab.js"><link rel="prefetch" href="/assets/js/22.cfe869c1.js"><link rel="prefetch" href="/assets/js/23.9b2e3461.js"><link rel="prefetch" href="/assets/js/24.1a5b67c9.js"><link rel="prefetch" href="/assets/js/25.b1797b07.js"><link rel="prefetch" href="/assets/js/27.51049fe1.js"><link rel="prefetch" href="/assets/js/28.ef4b2d59.js"><link rel="prefetch" href="/assets/js/29.c928f805.js"><link rel="prefetch" href="/assets/js/3.4563d256.js"><link rel="prefetch" href="/assets/js/30.bb455bd4.js"><link rel="prefetch" href="/assets/js/31.00048ba0.js"><link rel="prefetch" href="/assets/js/32.e70afc44.js"><link rel="prefetch" href="/assets/js/4.c15fd794.js"><link rel="prefetch" href="/assets/js/5.f8d772f8.js"><link rel="prefetch" href="/assets/js/6.c32ccf62.js"><link rel="prefetch" href="/assets/js/7.9372364e.js"><link rel="prefetch" href="/assets/js/vendors~docsearch.11055bee.js">
    <link rel="stylesheet" href="/assets/css/0.styles.13b65860.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar projects-page"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><!----> <span class="site-name">Mengmeng Wang</span></a> <div class="links"><!----> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link">
  Home
</a></div><div class="nav-item"><a href="/publications/" aria-current="page" class="nav-link router-link-exact-active router-link-active">
  Publications
</a></div><div class="nav-item"><a href="/projects/" class="nav-link">
  Projects
</a></div><div class="nav-item"><a href="/about/" class="nav-link">
  About
</a></div><div class="nav-item"><a href="https://www.zhihu.com/column/visual-tracking" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Blog
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link">
  Home
</a></div><div class="nav-item"><a href="/publications/" aria-current="page" class="nav-link router-link-exact-active router-link-active">
  Publications
</a></div><div class="nav-item"><a href="/projects/" class="nav-link">
  Projects
</a></div><div class="nav-item"><a href="/about/" class="nav-link">
  About
</a></div><div class="nav-item"><a href="https://www.zhihu.com/column/visual-tracking" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Blog
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav>  <!----> </aside> <main class="page"> <div class="theme-default-content content__default"><p><em>Here are details of some works ðŸ“š Thanks to all the co-authors for our works.</em></p> <h2 id="publications">Publications</h2> <h3 id="_2025">2025</h3> <ol><li>YAO Y, Chen J, Huang Z, Lin H, <strong>Wang M</strong>*, Dai G, Wang J. Manifold Constraint Reduces Exposure Bias in Accelerated Diffusion Sampling, International Conference on Learning Representations (<strong>ICLR</strong>), 2025.<strong>Corresponding Author</strong></li> <li><strong>Wang M</strong>, Ma T, Xin S, et al. Visual Object Tracking across Diverse Data Modalities: A Review[J]. arXiv preprint arXiv:2412.09991, 2024.</li> <li>Nan F, Tian F, Zhang N, Liu N, Miao H, Dai G, <strong>Wang M</strong>. Density-aware and Depth-aware Visual Representation for Zero-Shot Object Counting. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2025. <strong>Corresponding Author</strong></li> <li>Wang J, Yan C, Zhang W,  Lin H, <strong>Wang M</strong>, et al. SpotActor: Training-Free Layout-Controlled Consistent Image Generation[C]//Proceedings of the <strong>AAAI</strong> Conference on Artificial Intelligence. 2025.</li></ol> <hr> <h3 id="_2024">2024</h3> <ol><li>Lin H, Chen Y, Wang J, An W, <strong>Wang M</strong>*, et al. Schedule your edit: A simple yet effective diffusion noise schedule for image editing[C]. <strong>NeurIPS</strong>, 2024. <strong>Corresponding Author</strong></li> <li>Lin H, An W, Wang J, Y Chen, F Tian, <strong>Wang M</strong>, et al. Flipped classroom: Aligning teacher attention with student in generalized category discovery[C]. <strong>NeurIPS</strong>,  2024. <strong>(<font color="red">Oral</font>)</strong></li> <li>Wang J, Yan C, Lin H,  Zhang W, <strong>Wang M</strong>, et al. Oneactor: Consistent character generation via cluster-conditioned guidance[C]. <strong>NeurIPS</strong>,  2024.</li> <li><strong>Wang M</strong>, Xing J, Jiang B, et al. A Multimodal, Multi-Task Adapting Framework for Video Action Recognition[C]//Proceedings of the <strong>AAAI</strong> Conference on Artificial Intelligence. 2024, 38(6): 5517-5525.  <strong>(<font color="red">Oral</font>)</strong></li> <li>Lin H, <strong>Wang M</strong>, Chen Y, et al. DreamSalon: A Staged Diffusion Framework for Preserving Identity-Context in Editable Face Generation[J]. arXiv preprint arXiv:2403.19235, 2024. <strong>Corresponding Author</strong></li> <li>Jia C, Luo M, Chang X, Dang Z, Han M, <strong>Wang M</strong>,  et al. Generating action-conditioned prompts for open-vocabulary video action recognition[C]//Proceedings of the 32nd ACM International Conference on Multimediaï¼ˆ<strong>MM</strong>ï¼‰. 2024: 4640-4649.</li> <li>Ra J, <strong>Wang M</strong>, et al. Exploit Spatiotemporal Contextual Information for 3D Single Object Tracking via Memory Networks[C]//2024 International Conference on 3D Vision (<strong>3DV</strong>). IEEE, 2024: 842-851.</li> <li>Jia, C., Luo, M., Dang, Z., Dai, G., Chang, X., <strong>Wang, M.</strong>, &amp; Wang, J. (2024, March). Ssmg: Spatial-semantic map guided diffusion model for free-form layout-to-image generation. In Proceedings of the <strong>AAAI</strong> Conference on Artificial Intelligence (Vol. 38, No. 3, pp. 2480-2488).</li></ol> <hr> <h3 id="_2023">2023</h3> <ol><li><strong>Wang M</strong>, Xing J, Mei J, et al. ActionCLIP: Adapting Language-Image Pretrained Models for Video Action Recognition[J]. IEEE Transactions on Neural Networks and Learning Systems (<strong>TNNLS</strong>), 2023.</li> <li><strong>Wang M</strong>, Ma T, Zuo X, et al. Correlation pyramid network for 3d single object tracking[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition  (<strong>CVPR</strong>). 2023: 3215-3224.</li> <li>Xing J, <strong>Wang M</strong>, Ruan Y, et al. Boosting Few-shot Action Recognition with Graph-guided Hybrid Matching[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023: 1740-1750.</li> <li>Liu L, Song X, <strong>Wang M</strong>, et al. AGDF-Net: Learning Domain Generalizable Depth Features with Adaptive Guidance Fusion[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>), 2023.ðŸŽ‰ðŸŽ‰ðŸŽ‰</li> <li>Ma T, <strong>Wang M</strong>, Xiao J, et al. Synchronize Feature Extracting and Matching: A Single Branch Framework for 3D Object Tracking[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision (<strong>ICCV</strong>). 2023: 9953-9963.</li> <li>Chen, J., Bai, S., Huang, T., <strong>Wang, M</strong>., Tian, G., &amp; Liu, Y. (2023). Data-free quantization via mixed-precision compensation without fine-tuning. Pattern Recognition (<strong>PR</strong>), <em>143</em>, 109780.</li> <li>Xing J, <strong>Wang M</strong>, Mu B, et al. Revisiting the Spatial and Temporal Modeling for Few-shot Action Recognition[J]. <strong>AAAI</strong> 2023. <strong>Corresponding Author</strong></li> <li>Lv, J., Lang, X., Xu, J., <strong>Wang, M.</strong>, Liu, Y., &amp; Zuo, X. (2023). Continuous-time fixed-lag smoothing for lidar-inertial-camera slam. IEEE/ASME Transactions on Mechatronics (<strong>TMech</strong>).</li></ol> <hr> <h3 id="_2022">2022</h3> <ol><li><p><strong>Mengmeng Wang</strong>, Jiazheng Xing, Jing Su,  Jun Chen, Yong Liu*. Learning SpatioTemporal and Motion Features in a Unified 2D Network for Action Recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>), 2022.ðŸŽ‰ðŸŽ‰ðŸŽ‰</p></li> <li><p><strong>Wang, M</strong>., Mei, J., Liu, L., Tian, G., Liu, Y., &amp; Pan, Z. (2022). Delving Deeper Into Mask Utilization in Video Object Segmentation. <em>IEEE Transactions on Image Processing</em> (<strong>TIP</strong>), <em>31</em>, 6255-6266.</p></li> <li><p>Xu, C., Zhang, J., <strong>Wang, M</strong>., Tian, G., &amp; Liu, Y. (2022). Multilevel Spatial-Temporal Feature Aggregation for Video Object Detection. <em>IEEE Transactions on Circuits and Systems for Video Technology</em> (<strong>TCSVT</strong>), <em>32</em>(11), 7809-7820.</p></li> <li><p>Ma, T., Geng, S., <strong>Wang, M</strong>., Xu, S., Li, H., Zhang, B., ... &amp; Qiao, Y. (2022). Unleashing the Potential of Vision-Language Models for Long-Tailed Visual Recognition. <strong>BMVC</strong> 2022.</p></li> <li><p>Yang Y, <strong>Wang M</strong>, Mei J, et al. Exploiting semantic-level affinities with a mask-guided network for temporal action proposal in videos[J]. Applied Intelligence, 2022: 1-21.</p></li> <li><p>Lin H, <strong>Wang M</strong>, Liu Y, et al. Correlation-based and content-enhanced network for video style transfer[J]. Pattern Analysis and Applications, 2022: 1-13.</p> <hr></li></ol> <h3 id="_2021">2021</h3> <ol><li><strong>Wang, Mengmeng</strong>, Jiazheng Xing, and Yong Liu. &quot;Actionclip: A new paradigm for video action recognition.&quot; <em>arXiv preprint arXiv:2109.08472</em> (2021).</li> <li>Deng C, <strong>Wang M</strong>*, Liu L, et al. Extended feature pyramid network for small object detection[J]. IEEE Transactions on Multimedia (<strong>TMM</strong>), 2021.  <strong>corresponding author</strong></li> <li>Li Z, <strong>Wang M,</strong> Mei J, et al. Mail: A unified mask-image-language trimodal network for referring image segmentation[J]. arXiv preprint arXiv:2111.10747, 2021. <strong>Equal first contributor</strong>.</li> <li>Tian, G., Sun, Y., Liu, Y., Zeng, X., <strong>Wang, M</strong>., Liu, Y., ... &amp; Chen, J. (2021). Adding before pruning: Sparse filter fusion for deep convolutional neural networks via auxiliary attention. <em>IEEE Transactions on Neural Networks and Learning Systems</em> (<strong>TNNLS</strong>).</li> <li>Mei J, <strong>Wang M</strong>, Lin Y, et al. Transvos: Video object segmentation with transformers[J]. arXiv preprint arXiv:2106.00588, 2021.</li> <li>Huang, T., Zou, H., Cui, J., Yang, X., <strong>Wang, M</strong>., Zhao, X., ... &amp; Liu, Y. (2021). RFNet: recurrent forward network for dense point cloud completion. In <em>Proceedings of the IEEE/CVF international conference on computer vision</em> (<strong>ICCV</strong>) (pp. 12508-12517).</li> <li>Liu L, Song X, <strong>Wang M</strong>, et al. Self-supervised monocular depth estimation for all day images using domain separation[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision (<strong>ICCV</strong>). 2021: 12737-12746.</li> <li>Xiaoyang Lyu, Liang Liu, <strong>Mengmeng Wang</strong>, Xin Kong, Lina Liu, Yong Liu*, Xinxin Chen, Yi Yuan, HR-Depth : High Resolution Self-Supervised Monocular Depth Estimation,The Association for the Advance of Artificial Intelligence (<strong>AAAI</strong>), 2021</li> <li>Lina Liu, Xibin Song, Xiaoyang Lyu, Junwei Diao, <strong>Mengmeng Wang</strong>, Yong Liu*, Liangjun Zhang, FCFR-Net: Feature Fusion based Coarse-to-Fine Residual Learning for Depth Completion, The Association for the Advance of Artificial Intelligence (<strong>AAAI</strong>), 2021</li> <li>Jilin Tang, Yi Yuan*, Tianjia Shao, Yong Liu, <strong>Mengmeng Wang</strong>, Kun Zhou, Structure-aware Person Image Generation with Pose Decomposition and Semantic Correlation, the Association for the Advance of Artificial Intelligence (<strong>AAAI</strong>), 2021</li> <li>Guangming Yaoâ€ , Tianjia Shaoâ€ , Yi Yuan*, Shuang Li, Shanqi Liu, Yong Liu, <strong>Mengmeng Wang</strong>, Kun Zhou, One-shot Face Reenactment Using Appearance Adaptive Normalizationï¼Œthe Association for the Advance of Artificial Intelligence (<strong>AAAI</strong>), 2021</li> <li>Xu, C., Wu, X., Li, Y., Jin, Y., <strong>Wang, M</strong>*, &amp; Liu, Y. (2021). Cross-modality online distillation for multi-view action recognition. <em>Neurocomputing</em>, <em>456</em>, 384-393. <strong>corresponding author</strong></li></ol> <hr> <h3 id="before-2021">Before 2021</h3> <ol><li>Hao Zhang, <strong>Mengmeng Wang</strong>, Yong Liu*, Yi Yuan. FDN: Feature Decoupling Network for Head Pose Estimation, Proceedings of the 34th AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), New York, USA, 7-12 Feb. 2020.</li> <li>Zhang, J., Xu, C., Liu, L., <strong>Wang, M.</strong>, Wu, X., Liu, Y., &amp; Jiang, Y. (2020). Dtvnet: Dynamic time-lapse video generation via single still image. In <em>Computer Visionâ€“ECCV 2020: 16th European Conference, Glasgow, UK, August 23â€“28, 2020, Proceedings, Part V 16</em> (pp. 300-315). Springer International Publishing.</li> <li>Xianfang Zeng, Yusu Pan, <strong>Mengmeng Wang</strong>, Jiangning Zhang, Yong Liu*. Realistic Face Reenactment via Self-Supervised Disentangling of Identity and Pose, Proceedings of the 34th AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), New York, USA, 7-12 Feb. 2020</li> <li>Jiangning Zhang, Xianfang Zeng, <strong>Mengmeng Wang</strong>, Yusu Pan, Liang Liu,Yong Liu*, Yu Ding, Changjie Fan. FReeNet: Multi-Identity Face Reenactment, 2019 IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), Seattle, USA, 16 - 18 June, 2020ï¼Œ <strong>Equal First Author</strong></li> <li>Jiangning Zhang, Chao Xu, Lina Liu, <strong>Mengmeng Wang</strong>, Xia Wu, Yong Liu*, DTVNet: Dynamic Time-lapse Video Generation via Single Still Image, European Conference on Computer Vision (<strong>ECCV</strong>), 2020,</li> <li>Xianfang Zeng, Yusu Pan, Hao Zhang, <strong>Mengmeng Wang</strong>, Guanzhong Tian, Yong Liu*,  Unpaired Salient Object Translation via Spatial Attention Prior, Neurocomputing</li> <li>Kong, X., Yang, X., Zhai, G., Zhao, X., Zeng, X., <strong>Wang, M</strong>., ... &amp; Wen, F. (2020). Semantic graph based place recognition for 3d point clouds. In <em>2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (<strong>IROS</strong>)</em> (pp. 8216-8223).</li> <li>Boyuan Jiang, <strong>Mengmeng Wang</strong> *, Weihao Gan, Wei Wu, Junjie Yan. STM: SpatioTemporal and motion encoding for action recognition, Proceedings of the IEEE International Conference on Computer Vision (<strong>ICCV</strong>). 2019: 2000-2009. <strong>Corresponding Author</strong></li> <li><strong>Mengmeng Wang</strong>, Yong Liu*, Daobilige Su, Yufan Liao, Lei Shi and Jinhong Xu. Accurate and Real-time 3D Tracking for the Following Robots by Fusing Vision and Ultra-sonar Information. IEEE/ASME Transactions on Mechatronics, 2018, 23(3): 997 - 1006.ï¼ˆIF=4.943ï¼ŒSCIï¼‰</li> <li><strong>Mengmeng Wang</strong>, Yong Liu*, Zeyi Huang. Large Margin Object Tracking with Circulant Feature Maps, 2017 IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), Honolulu, Hawaii, 22-25 July, 2017.</li> <li><strong>Mengmeng Wang</strong>, Daobilige Su, Lei Shi, Yong Liu*, Jaime Valls Miro.	Real-Time 3D Human Tracking for Mobile Robots with Multisensors, 2017 IEEE International Conference on Robotics &amp; Automation (<strong>ICRA</strong>), Singapore, May 29-June 3, 2017.</li> <li><strong>Mengmeng Wang</strong>, Yong Liu, Rong Xiong. Robust object tracking with a hierarchical ensemble framework, 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (<strong>IROS</strong>), Korean, Oct.9 - Oct. 14, 2016, 2016: 438-445.</li></ol></div> <footer class="page-edit"><!----> <!----></footer> <!----> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.5f4e9a42.js" defer></script><script src="/assets/js/2.17befcb1.js" defer></script><script src="/assets/js/1.4fc49fe1.js" defer></script><script src="/assets/js/26.264cf966.js" defer></script>
  </body>
</html>
